---
title: "Reddit_Sentiment_Analysis"
author: "Kavya Deepthi Natta , Ajay Nawale"
date: "4/28/2020"
output: html_document
---

### **Title:**
* Reddit Sentiment Analysis


### **Team Members:**
* Kavya Deepthi Natta <br>
* Ajay Nawale


### **Business Context:**
* Reddit is a link sharing, discussion, and community building platform. The main content on reddit are “posts”, which can only be submitted by users with a reddit account. A “link post” consists of a title that links to any page on the web (including links to other pages on reddit). A “text post” consists of a title and body of text. Both link and text posts have a comments section where other users can discuss the post. An equally important component to the functionality of reddit is the system of “subreddits”. Subreddits are categories and every post is placed into a single subreddit. Redditors subscribe to subreddits that interest, allowing them to follow specific types of content more closely.


### **Problem Description**


### **Data Source**
* We have uploaded the data set in the google drive link that has been retrived using the reddit API package
* Link : https://drive.google.com/drive/folders/1LOzpr4vqfoyfuZ1zCOlo3xuqxa59dPmc?usp=sharing
* We used the RedditExtractorR package to extract the dataset.
* We have also extracted files from reddit pertaining to various vaccines like Pfizer,Morderna etc on which we have performed the word cloud to understand the data better.
* File Names are as follows : Reddit_comments.csv, Vaccines.csv.

### **Summarizing the dataset**
* The file that has been extracted from the Reddit has record count of '34768' which has mix comments related to various vaccines like Pfizer, Morderna and Astrazeneca 
* It has various features like post_id, post_author,post_title, post_score, comment(which is our most important feature), comment_date etc.
* We have also dropped few of the columns that were not contributing much to the sentiment analysis.

### **Importing Libraries**
```{r,message=FALSE, warning=FALSE}
library(tidyverse)
library(tibble)
library(ggplot2)
library(readr)
library(dplyr)
library(tidyr)
library(tidytext)
library(RColorBrewer)
library(reshape2)
library(wordcloud)
library(igraph)
library(widyr)
library(ggraph)
library(ngram)
library(wordcloud2)
library(stringr)
library(ggplot2)
library(tm)
library(wordcloud)
library(knitr)
library(kableExtra)
library(tidytext)
library(tidyverse)
library(tibble)
library(ggplot2)
library(readr)
library(dplyr)
library(tidyr)
library(tidytext)
library(RColorBrewer)
library(reshape2)
library(wordcloud)
library(igraph)
library(widyr)
library(ggraph)
library(ngram)
library(wordcloud2)
library(stringr)
library(ggplot2)
library(tm)
library(wordcloud)
library(knitr)
library(kableExtra)
library(data.table)
library(lubridate)
library(wordcloud)
```

### **Setting the working directory**
```{r,message=FALSE, warning=FALSE}
setwd(".")
getwd()
```

### **Reading the file**
```{r,message=FALSE, warning=FALSE}
file = "Reddit_Comments.csv"
df <- fread(file)

Vaccine_file = "Vaccines.CSV" 
df_Vaccine_file <- fread(Vaccine_file)

summary(df)
```

### **Glimpse of the dataset**
```{r,message=FALSE, warning=FALSE}
row1 <- head(df, 1)
row1
```

### **Data Summary**
#### 1.Total number of rows and columns in the dataset
```{r,message=FALSE, warning=FALSE}
nrow(df)
ncol(df)
```

#### 2.The datatype of each column
```{r,message=FALSE, warning=FALSE}
str(df)
```

### **Data cleansing**
#### Dropping column comment_edited as its not contributing much to the analysis
```{r,message=FALSE, warning=FALSE}
df$comment_edited <- NULL
```

#### Formatting time
```{r,message=FALSE, warning=FALSE}
df$post_date <- ymd_hms(df$post_date)
df$comment_date <- ymd_hms(df$comment_date)
```

#### Segregatting post_date into year, month and day which would be further used in the visualizations 
```{r,message=FALSE, warning=FALSE}
df$post_day <- wday(df$post_date,label = T)
df$post_month <- month(df$post_date,abbr = T,label = T)
df$post_year <- year(df$post_date)
(head(df,1))
```

#### Checking for nulls in the availble dataset
```{r,message=FALSE, warning=FALSE}
colSums(is.na(df))
```

#### **Summary of the dataset**
```{r,message=FALSE, warning=FALSE}
summary(df)
```

### **Data Visualization**
#### 1.Posts by year
```{r,message=FALSE, warning=FALSE}
ggplot(df[,.N,by=post_year],aes(x = post_year,y = N,fill=N, label=round(N,2)))+
  geom_bar(stat = "identity")+
  labs(title="Posts by year",subtitle="Number of Posts")+
  xlab("Year")+
  ylab(NULL)+
  geom_text(size=5, vjust=1, color="white")
```

#### Inference 
* It can be seen from the graph that the comments were higher during the mid of the year 2019 and it continued till the mid of 2020 next year. 

#### 2.Posts by month
```{r,message=FALSE, warning=FALSE}
ggplot(df[,.N,by=post_month],aes(x = post_month,y = N,fill=N, label=round(N,2)))+
  geom_bar(stat = "identity")+
  labs(title="Posts by month",subtitle="Number of Posts")+
  xlab("Month")+
  ylab(NULL)+
  geom_text(size=5, vjust=1, color="white")
```

#### Inference 
* It can be seen from the graph that the month of December had the highest number of comments. 


#### 3.Posts by Day
```{r,message=FALSE, warning=FALSE}
ggplot(df[,.N,by=post_day],aes(x = post_day,y = N,fill=N,label=round(N,2)))+
  geom_bar(stat = "identity")+
  labs(title="Posts by day",subtitle="Number of Posts")+
  xlab("Day")+
  ylab(NULL)+
  geom_text(size=5, vjust=1, color="white")
```

#### Inference 
* It can be seen from the graph that the Monday recorded the highest number of comments in comparison to the other days in a week. The lowest number of comments were recorded on Saturday


#### **Top 20 Authors as per the number of posts**
```{r,message=FALSE, warning=FALSE}
top_author<-df[,.N,by=comment_author][order(-N)][1:20]
top_author
```

#### **Top 20 Authors as per the posts score**
```{r,message=FALSE, warning=FALSE}
df[,.("Post_Score"=sum(post_score,na.rm=T)),by=comment_author][order(-Post_Score)][1:20]
```

### Cleaning the data of HTML tags, hashtags , punctuations and special characters
```{r,message=FALSE, warning=FALSE}
#Replacing the URLS
df_data <- df %>% mutate(comment_body = str_replace_all(comment_body, "//^(?:http(?:s)?:\\//\\//)?(?:[^\\.]+\\.)?[a-zA-Z0-9]\\.com(\\//.*)?$", ""))

df_Vaccine_file <- df_Vaccine_file %>% mutate(comment_body = str_replace_all(comment_body, "//^(?:http(?:s)?:\\//\\//)?(?:[^\\.]+\\.)?[a-zA-Z0-9]\\.com(\\//.*)?$", ""))

#Replacing HTML tags
df_data <- df_data %>% mutate(comment_body = str_replace_all(comment_body, "(<br />)+", ""))
df_Vaccine_file <- df_Vaccine_file %>% mutate(comment_body = str_replace_all(comment_body, "(<br />)+", ""))


#Remove # in hashtags
df_data <- df_data %>% mutate(comment_body = str_replace_all(comment_body, "#([^\\s]+)", "\1"))
df_Vaccine_file <- df_Vaccine_file %>% mutate(comment_body = str_replace_all(comment_body, "#([^\\s]+)", "\1"))


#Remove punctuations,numbers and special characters
df_data <- df_data %>% mutate(comment_body = str_replace_all(comment_body, "^[a-zA-Z0-9]*$", ""))
df_Vaccine_file <- df_Vaccine_file %>% mutate(comment_body = str_replace_all(comment_body, "^[a-zA-Z0-9]*$", ""))
```

### **NLP Procedure** 
#### The below procedure returns clean tokens. It also removes stop words and rare words that are part of the text

```{r,message=FALSE, warning=FALSE}
get_cleaned_tokens <- function(df_data,redditname) {
  if (redditname == 'all') {
    df_data <- df_data
  } else {
    df_data <- subset(df_data,subreddit == redditname)
  }
  
  tokens <- df_data %>% unnest_tokens(output = word, input = comment_body)
  tokens %>%  count(word,sort = TRUE)
  
  #get_stopwords()
  #get stop words
  sw = get_stopwords()
  #cleaned_tokens <- tokens %>%  anti_join(get_stopwords())
  cleaned_tokens <- tokens %>%  filter(!word %in% sw$word)
  
  nums <- cleaned_tokens %>% filter(str_detect(word, "^[0-9]")) %>% select(word) %>% unique()
  #head(nums)
  cleaned_tokens <- cleaned_tokens %>%   anti_join(nums, by = "word")
  #head(cleaned_tokens)
  
  cleaned_tokens %>%
    count(word, sort = T) %>%
    rename(word_freq = n) %>%
    ggplot(aes(x=word_freq)) +
    geom_histogram(aes(y=..count..), color="black", fill="blue", alpha=0.3) +
    scale_x_continuous(breaks=c(0:5,10,100,500,10e3), trans="log1p", expand=c(0,0)) +
    scale_y_continuous(breaks=c(0,100,1000,5e3,10e3,5e4,10e4,4e4), expand=c(0,0)) +
    theme_bw()
  
  rare <- cleaned_tokens %>%   count(word) %>%  filter(n<10) %>%  select(word) %>% unique()
  head(rare)
  
  rare <- cleaned_tokens %>%
    count(word) %>%
    filter(n<10) %>%
    select(word) %>% unique()
  
  alpha_remove <- cleaned_tokens %>% filter(str_detect(word, "^[Ã¢|s|t|r|gt|http]$")) %>%   select(word) %>% unique()
  
  
  cleaned_tokens <- cleaned_tokens %>%
    filter(!word %in% rare$word)
  length(unique(cleaned_tokens$word))
  
  cleaned_tokens <- cleaned_tokens %>% filter(!word %in% alpha_remove$word)
  return(cleaned_tokens)
}

```


### **Word Cloud for the reddit comments** 

```{r,message=FALSE, warning=FALSE}
cleaned_tokens = get_cleaned_tokens(df_data,'all')
cleaned_tokens_Pfizer = get_cleaned_tokens(df_Vaccine_file,'PfizerVaccine')
cleaned_tokens_Astrazeneca = get_cleaned_tokens(df_Vaccine_file,'Astrazeneca')
cleaned_tokens_Moderna = get_cleaned_tokens(df_Vaccine_file,'Moderna')
```


### **Plotting the word cloud related to Pfizer dataset** 

```{r,message=FALSE, warning=FALSE}
pal <- brewer.pal(8,"Dark2")
cleaned_tokens_Pfizer %>%
  count(word) %>%
  with(wordcloud(word, n, random.order = FALSE, max.words = 100, colors=pal))
```

### **Plotting the word cloud related to Astrazeneca dataset** 

```{r,message=FALSE, warning=FALSE}
pal <- brewer.pal(8,"Dark2")
cleaned_tokens_Astrazeneca %>%
  count(word) %>%
  with(wordcloud(word, n, random.order = FALSE, max.words = 100, colors=pal))
```

### **Plotting the word cloud related to Moderna dataset** 

```{r,message=FALSE, warning=FALSE}
pal <- brewer.pal(8,"Dark2")
cleaned_tokens_Moderna %>%
  count(word) %>%
  with(wordcloud(word, n, random.order = FALSE, max.words = 100, colors=pal))
```


### **Plotting the 100 most common words in the Reddit comments** 

```{r,message=FALSE, warning=FALSE}
pal <- brewer.pal(8,"Dark2")
cleaned_tokens %>%
  count(word) %>%
  with(wordcloud(word, n, random.order = FALSE, max.words = 100, colors=pal))
```


### **Sentiment Analysis** 
```{r,message=FALSE, warning=FALSE}
get_sentiments("nrc")
get_sentiments("afinn")
sent_reviews = cleaned_tokens %>%   
  left_join(get_sentiments("nrc")) %>%  
  rename(nrc = sentiment) %>%  
  left_join(get_sentiments("bing")) %>%  
  rename(bing = sentiment) %>%  
  left_join(get_sentiments("afinn")) %>%  
  rename(afinn = value)
```

### **Most positive and negative words** 
```{r,message=FALSE, warning=FALSE}
bing_word_counts <- sent_reviews %>%  
  filter(!is.na(bing)) %>%  
  count(word, bing, sort = TRUE)

head(bing_word_counts,5)
```

### **Graph of most positive and negative words** 
```{r,message=FALSE, warning=FALSE}
bing_word_counts %>%  
  filter(n > 700) %>% 
  mutate(n = ifelse(bing == "negative", -n, n)) %>%  
  mutate(word = reorder(word, n)) %>%  
  ggplot(aes(word, n, fill = bing)) +  geom_col() +  coord_flip() +  labs(y = "Contribution to sentiment")

```

#### Inference 
* It can be seen from the graph that the most positive words are good, like, pretty, effective,safe and the most negative words are risk, bad, death, problem, virus

### **Tokenizing by n-grams**
```{r,message=FALSE, warning=FALSE}
df_data$comment_body<-as.character(df_data$comment_body)
bigrams <- df_data %>%  unnest_tokens(bigram, comment_body, token = "ngrams", n = 2)
```

#### **Most common bi-grams**
```{r,message=FALSE, warning=FALSE}
bigrams %>%  count(bigram, sort = TRUE)
```

#### **Filtering by n-grams**
```{r,message=FALSE, warning=FALSE}
bigrams_separated <- bigrams %>%  
  separate(bigram, c("word1", "word2"), sep = " ")
bigrams_filtered <- bigrams_separated %>%  
  filter(!word1 %in% stop_words$word) %>%  
  filter(!word2 %in% stop_words$word)
```


#### **Bi-gram counts**
```{r,message=FALSE, warning=FALSE}
bigrams_filtered %>%   count(word1, word2, sort = TRUE)

bigram_united <- bigrams_filtered %>%
  unite(bigram, word1, word2, sep = " ")
bigram_counts <- bigram_united %>% 
  count(bigram, sort = TRUE)

bigram_counts=bigram_counts[-1,]
```

#### **Visualization of Top-10 bigram words**
```{r,message=FALSE, warning=FALSE}
bigram_counts %>% arrange(desc(n))%>% head(10)%>%ggplot(aes(x=factor(bigram,levels=bigram),y=n))+geom_bar(stat="identity",fill="#003E45")+labs(title="Top 10 bigram words")+coord_flip()

```

#### **Document Term Matrix**
```{r,message=FALSE, warning=FALSE}
word_counts_by_doc_id <- cleaned_tokens %>%  
  group_by(comment_id) %>%  
  count(word, sort = TRUE)

review_dtm <- word_counts_by_doc_id %>%  
  cast_dtm(comment_id, word, n)

review_dtm

#Topic Modeling using LDA

library(topicmodels)
lda8 <- LDA(review_dtm, k = 8, control = list(seed = 1234))
terms(lda8, 20)
lda8_betas <- broom::tidy(lda8)
lda8_betas


library(ggrepel)
terms_in_comments <- lda8_betas %>%  
  group_by(topic) %>%  
  top_n(5, beta) %>%  
  ungroup() %>%  
  arrange(topic, -beta)

terms_per_comments <- function(lda_model, num_words) {
  
  
  topics_tidy <- tidy(lda_model, matrix = "beta")
  top_terms <- topics_tidy %>%
    group_by(topic) %>%
    arrange(topic, desc(beta)) %>%
    slice(seq_len(num_words)) %>%
    arrange(topic, beta) %>%
    mutate(row = row_number()) %>%
    ungroup() %>%
    mutate(topic = paste("Comment_Topic", topic, sep = " "))
  title <- paste("LDA Top Terms for", k, "Comment_Topics")
  comments_wordchart(top_terms, top_terms$term, title)
}

comments_wordchart <- function(data, input, title) {
  data %>%
    
    ggplot(aes(as.factor(row), 1, label = input, fill = factor(topic) )) +
    
    geom_point(color = "transparent") +
    
    geom_label_repel(nudge_x = .2,  
                     direction = "y",
                     box.padding = 0.1,
                     segment.color = "transparent",
                     size = 3) +
    facet_grid(~topic) +
    theme_comments() +
    theme(axis.text.y = element_blank(), axis.text.x = element_blank(),
          axis.title.x = element_text(size = 9),
          panel.grid = element_blank(), panel.background = element_blank(),
          panel.border = element_rect("lightgray", fill = NA),
          strip.text.x = element_text(size = 9)) +
    labs(x = NULL, y = NULL, title = title) +
    
    ggtitle(title) +
    coord_flip()
}

theme_comments <- function() 
{
  theme(plot.title = element_text(hjust = 0.5),
        axis.text.x = element_blank(), 
        axis.ticks = element_blank(),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        legend.position = "none")
}
k <-8
terms_per_comments(lda8,15)
```

### **TF-IDF**
```{r,message=FALSE, warning=FALSE}
tfidf <- word_counts_by_doc_id %>%  
  bind_tf_idf(word, comment_id, n) 

#kable(head(tfidf,5))
head(tfidf,5)

```

* From TF-IDF we can infer the words  that are important in the particular document but not in the corpus

```{r,message=FALSE, warning=FALSE}
library(dplyr)
df_data %>% 
  select(comment_body) %>% 
  sentimentr::get_sentences() %>% 
  sentimentr::sentiment() %>% 
  mutate(characters = nchar(stripWhitespace(comment_body))) %>% 
  filter(characters >1 ) -> bounded_sentences 

summary(bounded_sentences$sentiment)

```

### **Extracting positive and negative sentiments**
```{r,message=FALSE, warning=FALSE}
bounded_sentences %>% filter(between(sentiment,-1,1)) ->  bounded_sentences
sentiment_densities <- with(density(bounded_sentences$sentiment), data.frame(x, y))
```

### **Visualization of sentiments from the Reddit Comments**
```{r,message=FALSE, warning=FALSE}
ggplot(sentiment_densities, aes(x = x, y = y)) +
  geom_line() +
  geom_area(mapping = aes(x = ifelse(x >=0 & x<=1 , x, 0)), fill = "green") +
  geom_area(mapping = aes(x = ifelse(x <=0 & x>=-1 , x, 0)), fill = "red") +
  scale_y_continuous(limits = c(0,2.5)) +
  theme_minimal(base_size = 16) +
  labs(x = "Sentiment", 
       y = "", 
       title = "Distribution of Sentiment Across Reddit comments") +
  theme(plot.title = element_text(hjust = 0.5), 
        axis.text.y=element_blank()) -> gg

plot(gg)
```

### **NLP summary**
Natural Language Processing allows machine to understand the text. 
We have implemented following in NLP:
* Tokenization helps us to cut the corpus into pieces known as tokens. For further processing, this tokens list is provided.

* Removing stop words is neccesary as the most frequent language words are stop words that must be filtered out in order to get more meaning out of them before data processing.

* Removing numbers is done as it is text analysis, We removed numbers because the analysis does not add enough meaning.

* Removing rare words is done in a corpus, seldom words usually occur less than 100. They don’t add any significance to the corpus, so I took them away.

* Sentiment analysis is done to find the positive and negative sentiments of text.

* Bigrams are used which is a sequence of 2 adjacent elements of a string of tokens. It is n=2 and are used for text analysis.

* Word correlations are used to find the correlated words so as to know the most related words by which we use to remove further uncommon words.

* Document Term Matrix is a mathematical matrix that describes the frequency of terms that occur in a collection of documents.

### **Final Remarks**

#### **References**
* We used the following references for our project

* [1] [R markdown](https://monashbioinformaticsplatform.github.io/2017-11-16-open-science-training/topics/rmarkdown.html)
* [2] [R markdown cheatsheet](https://github.com/rstudio/cheatsheets/raw/master/rmarkdown-2.0.pdf)
* [3] [Data cleaning](https://github.com/rfordatascience/tidytuesday/tree/master/data/2019/2019-04-23)
* [4] [ggplot2 Visualizations](http://r-statistics.co/Top50-Ggplot2-Visualizations-MasterList-R-Code.html#Ordered%20Bar%20Chart)
